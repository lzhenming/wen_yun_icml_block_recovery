\section{Analysis of our algorithm}\label{sec:analysis}
See Figure~\ref{fig:neighbor_algo} for our algorithm. This algorithm first constructs a graph $G$ on $[d]$ such that $\{i, j\} \in E(G)$ if and only if $|\hat \Sigma_{i,j}|$ is sufficiently large. Then the algorithm uses $|N(i) \cap N(j)|$ to determine whether $i$ and $j$ belongs to the same block. This section shows that $|N(i)\cap N(j)|$ concentrates at two different values for the case $i$ and $j$ in the same block and that $i$ and $j$ in different blocks. 

Sec~\ref{sec:firstmoment} analyzes the marginal distribution of $\hat \Sigma_{i, j}$; Sec~\ref{sec:secondmoment} analyzes 
the second moments $\hat \Sigma_{i, k} \cdot \hat \Sigma_{j, k}$ and gives tight bounds on $|N(i) \cap N(j)|$.

\subsection{Behavior of $\hat \Sigma_{i,j}$}\label{sec:firstmoment}

\begin{lemma}\label{lem:boundSigma} Let $i, j \in [d]$. Let $\calf_i$ be the $\sigma$-algebra generated by $M_{i, :}$, $\mX$, and $E_{i,:}$.
We have 1. $\hat \Sigma_{i,j} | \calf_i$ follows a Gaussian distribution, namely $N(0, V^2_{i,j})$, and 2. the value $V_{i,j}$ depends on whether $i$ and $j$ are in the same block. Specifically, let $\xi = \Theta(\log^2 d)$. There is a constant $c > 0$ such that
     
     \begin{itemize}
        \item If $i$ and $j$ are in the same block,  
            \begin{equation}
                n^2V^2_{i,j} \in_p (1\pm \frac{c\log d}{\ell})\sigma^4\sigma^4_x[n\ell(n + \ell +1) \pm \xi^2 n^2 \ell^{\frac{1}{2}}]. 
            \end{equation}
        \item If $i$ and $j$ are in different blocks, 
        \begin{equation}
             n^2V^2_{i,j} 
            \in_{p} (1 \pm \frac{c\log d}{\ell}) \sigma^4\sigma^4_x [n \ell^2 \pm \xi^2 n \ell^{\frac{3}{2}}].
         \end{equation}

    \end{itemize}
\end{lemma}

We interpret the meaning of Lemma~\ref{lem:boundSigma}. Let $i$ and $j$ be rows in the same block and $k$ be in a different block. Lemma~\ref{lem:boundSigma} asserts that \emph{(i)} $V^2_{i,j}$ is approximately $n/\ell$ times larger than$V^2_{i, j}$ with overwhelming probability{\color{red}by YW:incomplete here?}, and \emph{(ii)} the standard deviations highly concentrate and thus the probability that $\{i, j\} \in E(G)$ concentrates. 
We can thus use Lemma~\ref{lem:boundSigma} to tune 
$\tau$. Specifically, there exists a $c_0$ such that when we set $\tau = c_0\sqrt{\ell}${}, we roughly have $\Pr[\{i, j\} \in E(G)] \approx 1-\epsilon$ when $i$ and $j$ are in the same block and $\approx \epsilon$ otherwise
(see Corollary~\ref{cor:tunetau} in App.~\ref{asec:probedge} for an accurate statement \Znote{check}).

\iffalse
Before proceeding, let us interpret the meaning of Lemma~\ref{lem:boundSigma}. Let $i_1, j_1 \in B_1$ and $j_2 \in B_2$. One can see that $V_{i,j_2}$ is much smaller than $V_{i,j_1}$ (in the order of $n /\ell$). This means: 

{\color{blue} Insert by YW

The result here implies that $V^2_{i,j} = \Theta(\ell)$ when $i$ and $j$ are in the same block. It is intuitive in the sense that responses ($\mY$) from the same block are spanned by $\ell$ common random variables.

For the case that $i$ and $j$ are not in the same block, $V^2_{i,j} = \Theta(\frac{\ell^2}{n}) = \Theta(\frac{\ell}{n}\ell)$. (How to articulate the meaning of $\ell/n$?)
}
\fi

\begin{proof}[Proof of Lemma~\ref{lem:boundSigma}] 
We first analyze the case \textbf{
when $i$ and $j$ are in the same block.} 
Assuem $i, j \in B_1$. Let $\mX_{(1)} \in \reals^{n \times \ell}$ be a matrix that consists of columns of $\mX$ that belong to $B_1$. Let $\bar M_{i,:} \in \reals^{\ell}$ be a vector that consists of entries in $M_{i, :}$ that belong to $i$'s block.  
%{\color{red}by YW: maybe we can define $\calf$ in the beginning and use it here? Also, what about changing $i,j$ to $i,k$? so it accords with $\sigma_i$ in the last Chernoff bound.}
Conditioned on $\calf_i$, we have 
\begin{equation}
\begin{split}
    \hat{\Sigma}_{i,j}=\frac{1}{n} \left( \bar M_{i,:}\mX^{\transpose}_{(1)}+E_{i,:} \right)\left( \mX_{(1)}\bar M^{\transpose}_{j,:}+E^{\transpose}_{j,:} \right)\end{split}
\end{equation}
One can see that $\hat \Sigma_{i, j}$ is a linear combination of $\bar M_{j, :}$ and $E_{j,:}$ and therefore, it is also a Gaussian random variable with distribution $N(0, V^2_{i,j})$, where $V^2_{i,j}=\Var(\left. \hat{\Sigma}_{i,j} \right| M_{i,:},\mX,E_{i,:} )$.

We may compute $V_{i,j}$ in a brute-force manner. Specifically, 
\begin{equation}\label{eqn:expand}
\begin{split}
    n^2V^2_{i,j} = & \sigma^2 \bar M_{i, :} \mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :} \\
    & + 2\sigma^2 E^{\transpose}_{i,:} \mX_{(1)}\mX^{\transpose}_{(1)}\mX_{(1)}\bar M^{\transpose}_{i, :} \\
    & + \sigma^2 E^{\transpose}_{i,:}  \mX^{\transpose}_{(1)} \mX_{(1)}E_{i,:} + \sigma^2_{\epsilon} \|\mY_{i, :}\|^2_F.
\end{split}
\end{equation}

While (\ref{eqn:expand}) appears to be complex, simple calculation reveals that the first term ($\sigma^2 M_{i, :}\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}M^{\transpose}_{i, :}$) is the first order term (See Appendix~\ref{asec:prooflem41} for a complete analysis). 

We next analyze the term $\sigma^2 M_{i, :}\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}M^{\transpose}_{i, :}$. Let SVD of $\mX_{(1)}$ be $\mX_{(1)} = U_{(1)} \Sigma_{(1)} V^{\transpose}_{(1)}$, where  $U_{(1)}$ and $V_{(1)}$ are two (independent) random matrices consisting of orthogonal unit vectors.

Let $\vec m = V^{\transpose}_{(1)}\bar M^{\transpose}_{i,:}/\sigma$ be a vector in $\reals^{\ell}$. One can see that $\left. \vec m  \right| \mX \sim N(0,  I_{\ell})$. Thus, $\sigma^2\bar M_{i,:}V_{(1)}\Sigma^4_{(1)}V^{\transpose}_{(1)}\bar M^{\transpose}_{i,:}
    =\sigma^4 \sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)}).$ 
We have
{\small
\begin{equation}
\begin{split}
    \E \left[\sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)})\right]
    &=  \E \left[\E\left[\sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)})|\mX\right]\right]  \\ 
    &= \E \left[\|\mX_{(1)}^{\transpose}\mX_{(1)}   \|^2_F \right]   
    %&= \sigma^4_x[\ell(2n+n^2)+(\ell^2 - \ell)n] \\ 
    = \sigma^4_x n\ell(n + \ell +1)
\end{split}
\end{equation}
}
By Lemma~\ref{lem:Fnorm} and Lemma~\ref{lem:randommat},
\begin{equation}
    \vec m^2_{t}\sigma^4_{t}(\mX_{(1)})
    \leq_p  \sigma^4_x (1+\xi)(\sqrt{n}+\sqrt{\ell}+\xi)^4.
\end{equation}

Let $F_1 \equiv \sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)})$. We may use Hoeffding's inequality and get $\Pr\left[\left|F_1 - \E[F_1] \right|\geq \sigma^4_x\xi^2 n^2 \ell^{\frac{1}{2}}
    \right] \leq 2\exp\{-\Theta(\xi^2)\}$. 
So the leading term is $\sigma^4\sigma^4_x[n\ell(n + \ell +1) \pm \xi^2 n^2 \ell^{\frac{1}{2}}]$.


\iffalse
\begin{equation}
\begin{split}
    &\Pr\left[\left|F_1 - \E[F_1] \right|\geq \sigma^4_x\xi^2 \ell^{\frac{5}{2}}
    \right] \\
    \leq& 2 \exp \left(-\frac{2\xi^4\ell^5}{\ell(1+\xi)^2(\sqrt{n}+\sqrt{\ell}+\xi)^8 } \right) \\ 
    =&  2\exp\{-\Theta(\xi^2)\}
\end{split}
\end{equation}
\fi
    
\iffalse
In addition, by Lemma~\ref{lem:randommat}, 

\begin{equation}\label{eqn:extreme}
(\sqrt{n} - \sqrt{\ell} - \xi)\sigma_x \leq_p \sigma_{\min}(\mX_{(1)}) \leq \sigma_{\max}(\mX_{(1)}) \leq_p (\sqrt{n} + \sqrt{\ell} + \xi)\sigma_x. 
\end{equation}

Let $\vec m = V^{\transpose}_{(1)}\bar M^{\transpose}_{i,:}$ be a vector in $\reals^{\ell}$. One can see that $\left. \vec m  \right| \mX \sim N(0, \sigma^2 I_{\ell})$. Thus, $\sigma^2\bar M_{i,:}V_{(1)}\Sigma^4_{(1)}V^{\transpose}_{(1)}\bar M^{\transpose}_{i,:}
    =\sigma^2 \sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)}).$  This implies
{\small
\begin{equation}
        \sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)})
        \leq \sigma^4_{\max}(\mX_{(1)})\sum^{\ell}_{t = 1}\vec m^2_{t} 
         = \sigma^4_{\max}(\mX_{(1)})\| \vec m\|^2_2
\end{equation}
}
and  
\begin{equation}
        \sum^{\ell}_{t = 1}\vec m^2_{t}\sigma^4_{t}(\mX_{(1)})
         \geq \sigma^4_{\min}(\mX_{(1)})\sum^{\ell}_{t = 1}\vec m^2_{t}  = \sigma^4_{\min}(\mX_{(1)})\|\vec m\|^2_2.
\end{equation}

 
By using a simple Chernoff bound (Lemma~\ref{lem:Fnorm}), we have $\|m\|^2_2 \in_p (\ell \pm \xi \sqrt{\ell})\sigma^2$.

Together with (\ref{eqn:extreme}),  we prove the first part of the Lemma.
\fi

We next analyze the case \textbf{when $i$ and $j$ are in different blocks.} Assume $i \in B_1$ and $j \in B_2$. Recall that $V^2_{i,j} = \Var(\left. \hat{\Sigma}_{i,j} \right| M_{i,:},\mX,E_{i,:} )$. We have

\begin{equation}
\begin{split}
    n^2 V^2_{i,j} 
    =&  \sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(2)}\mX_{(2)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :}   \\
    &+ 2\sigma^2E_{i,:}\mX_{(2)}\mX^{\transpose}_{(2)}\mX_{(1)}\bar M^{\transpose}_{i,:}  \\
    &+\sigma^2 E_{i,:}\mX_{(2)} \mX_{(2)}^{\transpose} E^{\transpose}_{i,:} + \sigma^2_{\epsilon}\|\mY_{i,:}\|^2_F 
\end{split}
\end{equation}

We will again focus on analyzing the dominating term $\sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(2)}\mX_{(2)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :}$ (See App.~\ref{asec:prooflem41} for a complete analysis). \textbf{Our crucial observation is that $\sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(2)}\mX_{(2)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :}$ is $\ell/n$ times smaller than $\sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :}$} and this is the key reason that $V_{i,j}$ is much smaller when $i$ and $j$ are in different blocks. 

Let the SVD of $\mX_{(1)}$ be $\mX_{(1)} = U_{(1)}\Sigma_{(1)}V^{\transpose}_{(1)}$ and $B \equiv U^{\transpose}_{(1)}\mX_{(2)} /\sigma_x $. We have %where $U_{\mX_{(1)}}\in \reals^{n\times \ell}$ satisfies $U^{\transpose}_{\mX_{(1)}}U_{\mX_{(1)}} = I_{\ell}$, $V_{\mX_{(1)}} \in R^{\ell \times \ell}$ is a orthogonal matrix and $\Sigma_{\mX_{(1)}} \in R^{\ell \times \ell}$ is a diagonal matrix with singular values of $\mX_{(1)}$ on the diagonal.
{\small
\begin{equation}
\begin{split}
   & \sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(2)}\mX_{(2)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{i, :}  
     =  \sigma^4 \sigma^2_x \vec m^{\transpose}\Sigma_{(1)}B B^{\transpose}\Sigma_{(1)}\vec m
\end{split}
\end{equation}
}
We now analyze the behavior of $B$. Because $U_{(1)}$ is an orthonormal matrix and $\mX_{(2)}$ is a random Gaussian matrix, we can see that $B|\mX_{(1)}$ consists of i.i.d Gaussian random variables with distribution $N(0, 1)$. 

\myparab{$\ell/n$-gap comes from random projection.} Before proceeding, we shall comment on this operation. $U^{\transpose}_{(1)}\mX_{(2)}$ can be viewed as projecting column vectors of $\mX_{(2)}$ (in $\reals^{n}$) onto a subspace spanned by the (orthonormal) columns of $U_{(1)}$. The distribution of $U^{\transpose}_{(1)}$ is the same as randomly sample $\ell$ orthonormal vectors from $\reals^{n}$~\cite{chafai2009singular}. This means we project $\mX_{(2)}$ into a \emph{random subspace of dimension $\ell$}. 
This operator shrinks each column in $\mX_{(2)}$ by an order of $\ell/n$, and it is the root cause for $V_{i,j}$ being small when $i$ and $j$ are in different blocks. 



%Based on the above analysis, any pair of entries in $U^{\transpose}_{\mX_{(1)}}\mX_{(2)}$ is uncorrelated and thus independent when $\mX_{(1)}$ is given. I.e., conditioned on $\mX_{(1)}$, $U^{\transpose}_{\mX_{(1)}}\mX_{(2)}/\sigma_x$ is a random matrix with entries following i.i.d. standard Gaussian distribution. 
%An intuitive way to understand this part is: by projecting vectors in $R^n$ onto $\ell$ orthogonal vectors, the magnitude is reduced in order of $\ell/n$.




\iffalse
We now continue our analysis. Note that $B$ is symmetric. Let the eigendecomposition of $B$ be $B = Q_{B}\Lambda_{B} Q^{\transpose}_{B}$. 
Let $\lambda_t(B)$ be the $t$-th largest eigenvalue of $B$. We have $\lambda_t(B) 
    = \sigma^2_t\left(\frac{U^{\transpose}_{(1)}\mX_{(2)}}{\sigma_x}\right) 
    \geq 0,$
where $\sigma_t$ is the $t$-th largest singular value of $\frac{U^{\transpose}_{(1)}\mX_{(2)}}{\sigma_x}$.

Let $\vec m = Q^{T}_{B} \Sigma_{(1)}V^{\transpose}_{(1)} \bar M^{\transpose}_{i,:}$, where $\left. \vec m  \right| \mX \sim N\left(0, \sigma^2 \Sigma^2_{(1)}\right)$. Let $\Tilde{m} \equiv \frac{\vec m_{t}}{\sigma \dot \sigma_t(\mX_{(1)})}$. Note that the entries $\Tilde{m}_t$ are i.i.d. standard Gaussian random variables. We have 
\fi


Let $F_2 \equiv \vec m^{\transpose}\Sigma_{(1)}B B^{\transpose}\Sigma_{(1)}\vec m$. The SVD of $B$ is $U\Sigma(B)V^{\transpose}$, then
\begin{equation} \label{eqn:expect_F2}
\begin{split}
     \E[F_2]
     &= \sum^{\ell}_{t=1} \E\left[\E\left[\left.\left(\sum^{\ell}_{s=1}\vec m_s \sigma_s(\mX_{(1)})U_{s,t}\sigma_t(B)\right)^2 \right| \mX \right]\right] \\ 
     %&= \sum^{\ell}_{t=1} \E\left[\sum^{\ell}_{s=1} \sigma^2_s(\mX_{(1)})U^2_{s,t}\sigma^2_t(B) \right]  \\ 
     %&=\sum^{\ell}_{t=1} \sum^{\ell}_{s=1}\E\left[ \sigma^2_s(\mX_{(1)})\right] \E \left[U^2_{s,t}\sigma^2_t(B)|\mX_{(1)} \right]  \\ 
     &= \sum^{\ell}_{s=1} \left( \E\left[\sigma^2_s(\mX_{(1)}) \right]\sum^{\ell}_{t=1} \E\left[U^2_{s,t}\sigma^2_t(B)|\mX_{(1)} \right]\right)  
\end{split}
\end{equation}

Note that $U_{s,t}$ is the link between the subscripts of two singular values ($\sigma^2_s(\mX_{(1)}),\sigma^2_t(B)$) and $B|\mX_{(1)}$ is independent of $\mX_{(1)}$, so by symmetry, we can change the the subscript of $U_{s,t}$ but still get the same result. Specifically,
let $s' \equiv s + v \mod \ell$, where $v = 1,2,\cdots, \ell$, we have
\begin{equation} \label{eqn:expect_n}
\begin{split}
    \E[F_2] 
    &= \sum^{\ell}_{s=1} \left( \E\left[\sigma^2_s(\mX_{(1)}) \right]\sum^{\ell}_{t=1} \E\left[U^2_{s',t}\sigma^2_t(B)|\mX_{(1)} \right]\right) \\ 
    &= \frac{1}{\ell} \sum^{\ell}_{s=1} \left( \E\left[\sigma^2_s(\mX_{(1)}) \right]\sum^{\ell}_{t=1}\sum^{\ell}_{v=1} \E\left[U^2_{v,t}\sigma^2_t(B)|\mX_{(1)} \right]\right) \\ 
    &= \frac{1}{\ell} \sum^{\ell}_{s=1} \left( \E\left[\sigma^2_s(\mX_{(1)}) \right]\ell^2 \right) 
    = n\ell^2
\end{split}
\end{equation}

Finally, by  Lemma~\ref{lem:randommat} and Lemma~\ref{lem:Fnorm} and a Hoeffding's inequality, we have
\begin{equation}
\begin{split}
    \Pr\left[\left|F_2 - \E[F_2] \right|\geq \sigma^4_x\xi^2 n\ell^{\frac{3}{2}}
    \right] \leq 2\exp\{-\Theta(\xi^2)\}
\end{split}
\end{equation}
So the leading term is in $\sigma^4\sigma^4_x [n \ell^2 \pm \xi^2 n\ell^{\frac{3}{2}}]$ with overwhelming probability.



\iffalse
Next, $F_2 = \sum^{\ell}_{t=1} \sum^{\ell}_{s=1} \vec m_s^2 \sigma^2_s(\mX_{(1)})U^2_{s,t}\sigma^2_t(B) $ and by Lemma~\ref{lem:randommat} and Lemma~\ref{lem:Fnorm} we have
\begin{equation}
\begin{split}
    &\sum^{\ell}_{s=1} \vec m_s^2 \sigma^2_s(\mX_{(1)})U^2_{s,t}\sigma^2_t(B) \\
    \leq_p& \sigma^4_x (\sqrt{n}+\sqrt{\ell}+\xi)^2(1+\xi)\sum^{\ell}_{s=1}  U^2_{s,t}\sigma^2_t(B)   \\ 
    \leq_p &\sigma^4_x (\sqrt{n}+\sqrt{\ell}+\xi)^2 (2\sqrt{\ell}+\xi)^2 (1+\xi)
\end{split}
\end{equation}
By Hoeffding's inequality,we have
\begin{equation}
\begin{split}
    &\Pr\left[\left|F_2 - \E[F_2] \right|\geq \sigma^4_x\xi^2 \ell^{\frac{5}{2}}
    \right] \\
    \leq& 2 \exp \left(-\frac{2\xi^4\ell^5}{\ell(1+\xi)^2(\sqrt{n}+\sqrt{\ell}+\xi)^4(2\sqrt{\ell}+\xi)^4} \right) \\ 
    =&  2\exp\{-\Theta(\xi^2)\}
\end{split}
\end{equation}


{\color{red}by YW: I have changed the proof and some notations, and then we may need to update the following intuition.}
We next need to develop a concentration bound on $\sum^{\ell}_{t = 1}\lambda_{t}(B)\Tilde{m}^2_t$ conditioned on $\mX$. $\lambda_{t}(B)$'s are measurable by $\sigma(\mX)$ and thus can be treated as constants. On the other hand, $\tilde m_t$ are i.i.d Gaussian. Here, we use the standard trick to first cut off the tail of each $\tilde m_t$. For example, $\tilde m_t \leq_p \log^2 d$. Then we may use a Hoeffding inequality (see {\color{red} XXX in Appendix XX}) to bound $\sum^{\ell}_{t = 1}\lambda_{t}(B)\Tilde{m}^2_t$. One interesting observation is that the quality of the Hoeffding inequality is parametrized by $\sum^{\ell}_{t = 1}\lambda^2_{t}(B) = \|B\|^2_F$. While we do not have concentration result for each $\lambda^2_t$ (the concentration result in Lemma~\ref{lem:randommat} is only applicable to extreme values), it is possible to get a concentration for $\|B\|^2_F$. {\color{red} See Appendix X for a full analysis.}

\fi

\end{proof}



\myparab{Story so far.} The graph $G$ we construct ``approximately'' produces the block structure with constant error probability: when $i$ and $j$ 
are in the same block, $\Pr[\{i, j\} \in E(G)] \approx 1 - \epsilon$; when $i$ and $j$ are in different blocks. But it is impossible to push down the constant error any further. This is a key reason we need to generalize graph-based learning algorithms~\cite{rohe2011spectral,abraham2015low} to leverage ``global information'' to identify blocks. See the section below.  

