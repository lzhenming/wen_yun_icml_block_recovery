
\subsection{Behaviors of $\hat \Sigma_{i,k} \hat \Sigma_{j,k}$}\label{sec:secondmoment}

This section analyzes the interactions between $\hat \Sigma_{i,k}$ and $\hat \Sigma_{j,k}$. Because all blocks are symmetric, we may wlog assume $i$, $j$, and $k$ are all in $B_1 \cup B_2 \cup B_3$. Define $\calf_{i,j}$ be the $\sigma$-algebra generated by $M_{i, :}$, $M_{j, :}$, $E_{:, i}$, $E_{:, j}$, and $\mX$. When the context is clear, we write $\calf$ instead of $\calf_{i,j}$. 
To simplify the notation, we write the conditional random variables $K_i = \hat \Sigma_{i, k} \mid \calf$ and $K_j = \hat \Sigma_{j, k} \mid \calf$. Below is our main proposition. 

\begin{proposition}\label{prop:interaction} For any small positive constants $\epsilon  < 0.5$, there exists a constant $c_0$ and $\tau$ such that when $n > c_0 \ell$, 
{\small
\begin{equation}\label{eqn:correlatedevent}
    \begin{split}
        & \Pr[|K_i| > \tau \wedge |K_j| > \tau] \\
       & \in_p \left\{
       \begin{array}{ll}
        (1-\epsilon)^2 \pm O(\xi^2/\sqrt{\ell})     & \mbox{ when $i,j, k \in B_1$ } \\
        \epsilon^2 \pm O(\xi^2/\sqrt{\ell})    & \mbox{ when $i,j \in B_1$ and $k \in B_2$} \\
        (1-\epsilon)\epsilon \pm O(\xi^2/\sqrt{\ell}) & \mbox{when $i,k \in B_1$ and $j \in B_2$}\\
        \epsilon^2 \pm O(\xi^2/\sqrt{\ell}) & \mbox{when $i \in B_1$, $j \in B_2$ and $k \in B_3$} 
       \end{array}
       \right. 
    \end{split}
\end{equation}
}
\end{proposition}
\iffalse
$\Pr[|K_i| > \tau \wedge |K_j| > \tau\calf]$ can be bounded 
\begin{enumerate}
    \item When $i,j, k \in B_1$: $(1-\epsilon)^2 \pm O(\xi^2/\sqrt{\ell}) 
    $. 
    \item When $i,j \in B_1$ and $k \in B_2$: $\epsilon^2 \pm O(\xi^2/\sqrt{\ell}) 
    $. 
    \item When $i,k \in B_1$ and $j \in B_2$: $(1-\epsilon)\epsilon \pm O(\xi^2/\sqrt{\ell})
    $. 
    \item When $i \in B_1$, $j \in B_2$ and $k \in B_3$: $\epsilon^2 \pm O(\xi^2/\sqrt{\ell}) 
    $. 
\end{enumerate}
\fi
We make a few remarks. (i) for all four cases, the first terms in the right hand side are the dominating terms and the second terms are in the order of $O(\frac{n \log^2d}{\ell^3}) = \tilde O(1/\ell^2)$. (ii) case 2 and case 3 in (\ref{eqn:correlatedevent}) are two different cases because of $\calf$ contains $M_{i,:}$ and $M_{j, :}$ but not $M_{k, :}$. 

\myparab{Proof roadmap.} $[K_i,K_j]$ is a bivariate Gaussian random variable. In general, it is difficult to approximate $\Pr[|K_i| > \tau \wedge |K_j| > \tau]$. Here, we argue that $K_i$ and $K_j$ are ``approximately independent'' so it can be approximated by $\Pr[|K_i| > \tau]\Pr[|K_j| > \tau]$. Our analysis consists of three steps.
\emph{Step 1. Decomposition.}  We decompose $K_j$ as $K_j = \alpha K_i + K_i^{\bot}$, where $K_i^{\bot}$ is a Gaussian r.v. independent of $K_i$. We will show that $\alpha$ is a small order term. \emph{Step 2. Approximation.} We next will leverage the approximately independent property ($\alpha$ is small) to reduce the analysis of correlated events into analyses of tails of independent variables. 
\emph{Step 3. Apply first moment results.} Finally, we use results developed in Section~\ref{sec:firstmoment} to give  bounds for $\Pr[|K_i| > \tau \wedge |K_j| > \tau]$.  

%We now walk through each of the steps. The full analysis is available in {\color{red} Appendix X.} 

\myparab{Step 1. Decomposition.} Let $K_j = \alpha K_i + K_i^{\bot}$, where $K_i^{\bot}$ is a Gaussian r.v. independent of $K_i$, we have
$\alpha = \frac{\E[K_i K_j]}{\E[K^2_i]}$. Our goal is to show that $\alpha$ is in the order of $\tilde O(1/\sqrt{\ell})$. One can use a standard Chernoff bound to argue that $K^2_i$ is in the order of $\Theta(\ell)$. Thus, we need only show the following lemma.  

\begin{lemma}\label{lem:angle} Using the notations above, for any $i$, $j$, and $k$, we have $\E[K_i K_j] =_p O((\log d) \sqrt{\ell})$. Furthermore, $|\alpha| =_p O\left(\frac{\log d}{\sqrt {\ell}}\right).$
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:angle}]
 We shall prove the case where $i, j, k \in B_1$. The rest of the cases can be analyzed in a similar manner and is presented in {\color{red} Appendix X} {\color{blue}by YW on 20190120: finished in Appendix}. We have 
 {\small
\begin{equation}
\begin{split}
    & \E[n^2 K_i K_j] \\
    &=  \sigma^2 \bar M_{i, :}\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{j, :}   + \sigma^2E_{i,:}\mX_{(1)}\mX^{\transpose}_{(1)}\mX_{(1)}\bar M^{\transpose}_{j,:}  \\
    &+ \sigma^2E_{j,:}\mX_{(1)}\mX^{\transpose}_{(1)}\mX_{(1)}\bar M^{\transpose}_{i,:}
    +\sigma^2 E_{i,:}\mX_{(1)} \mX_{(1)}^{\transpose} E^{\transpose}_{j,:} + \sigma^2_{\epsilon}\mY_{i,:}\mY^{\transpose}_{j,:}
\end{split}
\label{eqn:correlatedinteraction}
\end{equation}
}
The dominating term is still the first term. A key difference between (\ref{eqn:correlatedinteraction}) and the analysis in Lemma~\ref{lem:boundSigma} is that we study the interaction between $\bar M_{i, :}$ and $\bar M_{j, :}$ here whereas in Lemma~\ref{lem:boundSigma} we study the interaction between $\bar M_{i, :}$ and itself. The major trick here is that we choose the conditioning carefully so that we can use the fact Gaussian's are 2-stable~\cite{indyk2000stable}. Specifically, let $\mX = U_{(1)}\Sigma_{(1)}V^{\transpose}_{(1)}$ be the SVD of $\mX$. Now conditioned on $\mX$ and $\bar M_{j, :}$, $\bar M_{i,:}(\mX^{\transpose}\mX \mX^{\transpose} \mX) \bar M^{\transpose}_{j, :}$ is a Gaussian random variable with standard deviation
\begin{equation}
\begin{split}   \label{eqn:temp2}
    & \sigma(\bar M_{i,:}(\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)})\bar M^{\transpose}_{j, :} \mid M_{j, :} \mX_{(1)}) \\
    &= \sigma \|\mX_{(1)}^{\transpose}\mX_{(1)}\mX_{(1)}^{\transpose}\mX_{(1)}\bar M^{\transpose}_{j,:}\|_2   \\ 
    &= \sigma \|V_{(1)}\Sigma^4_{(1)}V_{(1)}^{\transpose}\bar M^{\transpose}_{j,:}\|_2 = \sigma \|\Sigma^4_{(1)}V_{(1)}^{\transpose}\bar M^{\transpose}_{j,:}\|_2  \\
    & \leq \sigma \sigma_{\max}^4(\mX_{(1)})\|\bar M^{\transpose}_{j,:}\|_2,
\end{split}
\end{equation}
where $\sigma_{\max}(\mX_{(1)})$ is the largest singular value of $\mX_{(1)}$. 
We may apply Lemma~\ref{lem:randommat} to bound $\sigma_{\max}^4(\mX_{(1)})$ and a standard concentration result to bound $\|\bar M^{\transpose}_{j,:}\|_2$ (entries in $\bar M^{\transpose}_{j,:}$ are all independent Gaussian), and conclude that $\E[K_i K_j] \in_p O\left( (\log d) \sqrt{\ell} \right)$. 
\end{proof}


\myparab{Step 2. Approximation.} This step aims to calculate $\Pr[|K_i| > \tau \wedge |K_j| > \tau]$. Let $\sigma_{i, \bot}$ be the standard deviation of $K^{\bot}_i$. 

\begin{lemma}\label{lem:calculate} Using the notations above, when $\frac{\tau}{\sigma_{i,\bot}} \leq \sqrt{\frac{2}{2\alpha + \alpha^2}}$,


$    \Pr\left[|K_j |>\tau \wedge | K_i | > \tau \right]
   \in_p 
   \Pr[|K^{\bot}_i| > \tau ]\Pr[|K_i| > \tau ] + \left[-\frac{f_{K^{\bot}_i}(\tau )\alpha^2 \tau}{\sigma^2_{i,\bot}}, 0 \right]
$
\end{lemma}


Lemma~\ref{lem:calculate} is consistent with our intuition. Because $K_i$ and $K_j$ are approximately independent, we have $\Pr[|K_i| > \tau \wedge |K_j| > \tau] \approx \Pr[|K^{\bot}_i| > \tau ]\Pr[|K_i| > \tau]$. Proving Lemma~\ref{lem:calculate} requires heavy algebra manipulating. Here, we highlight a few key steps and defer the complete analysis to {\color{red} Appendix X.}{\color{blue}by YW on 20190120: finished in Appendix.} 

\begin{proof}[Outline of proof for Lemma~\ref{lem:calculate}]
Because $K_i$ and $K_j$ are symmetric, we may wlog assume that $0 \leq \alpha \leq 1$. 

We have
\begin{equation}
\begin{split}
   & \Pr[|K_i| > \tau \wedge |K_j| > \tau ]\\
    &= \Pr[|K_j| > \tau] - \Pr[|K_i| \leq \tau \wedge |K_j| > \tau ]    \\ 
     &=\Pr[|K_j| > \tau] - \Pr[ |K_j| > \tau | |K_i| \leq \tau]\Pr[ |K_i| \leq \tau]
\end{split}
\end{equation}
Now the key is to show that $\Pr[ |K_j| > \tau | |K_i| \leq \tau] \approx \Pr[ |K^{\bot}_i| > \tau]$. We have
\begin{equation} \label{eqn:temp11}
\begin{split}
    &\Pr\left[|K_j |>\tau \wedge | K_i | \leq \tau \right]\\
    &= 2 \int_0^{\tau} \Pr\left[|K_j |>\tau \Big | K_i = x\right]f_{K_i}(x) dx,
\end{split}
\end{equation}
where $f_{K_i}$ is the pdf of Gaussian r.v. $K_i$. We next focus on analyzing $\Pr[|K_j|> \tau | K_i = x]$ when $0 \leq x \leq \tau$:
{\small
\begin{equation}\label{eqn:lowerprobmain}
\begin{split}
    & \Pr[|K_j|> \tau | K_i = x]\\ 
    =&\Pr[(K^{\bot}_i > \tau - \alpha x) + \Pr[(K^{\bot}_i < - \tau - \alpha x)]   \\ 
    =& \Pr[K^{\bot}_i > \tau] + \Pr[K^{\bot}_i < - \tau] \\ 
    &+ \Pr[\tau - \alpha x \leq K^{\bot}_i \leq \tau] - \Pr[-\tau - \alpha x \leq K^{\bot}_i \leq -\tau]  \\ 
    =& \Pr[|K^{\bot}_i| > \tau] +  \int_0^{\alpha x}f_{K^{\bot}_i}(\tau - \alpha x+z) - f_{K^{\bot}_i}(-\tau - z)dz  \\ 
    =& \Pr[|K^{\bot}_i| > \tau] +  \int_0^{\alpha x}f_{K^{\bot}_i}(\tau - \alpha x+z) - f_{K^{\bot}_i}(\tau + z)dz,
\end{split}
\end{equation}
}
where $f_{K^{\bot}_i}$ is the pdf of $K^{\bot}_i$. 
(\ref{eqn:lowerprobmain}) says that we may express $\Pr[|K_j|> \tau | K_i = x]$ as a term $\Pr[|K^{\bot}_i|> \tau]$  that is independent of $K_i$ and an error term that's a function of $\alpha$. 

Roughly speaking, we use the locally linear property of a Gaussian distribution's pdf (through Taylor expansion). Therefore, this difference can be controlled by $\alpha x = o(1)$.  Specifically, we can see that 
\begin{equation}
    \int_0^{\alpha x}f_{K^{\bot}_i}(\tau - \alpha x+z) - f_{K^{\bot}_i}(\tau + z)dz  \leq \frac{2f_{K^{\bot}_i}(\tau )\alpha^2 x^2 \tau}{\sigma^2_{i,\bot}}. 
\end{equation}
 Now using the fact that 
$K^{\bot}_i = K_j - \alpha K_i$ and Lemma(\ref{lem:angle}), we may complete the proof of Lemma~\ref{lem:calculate}.
\end{proof}

\myparab{Step 3. Application of first moment result.} 
We have
\begin{equation}\label{eqn:decouplebnd}
\begin{split}
    &\Pr[|K_i| > \tau \wedge |K_j| > \tau]\\
    &\in_p 4\Phi\left(-\frac{\tau}{\sigma_{i,\bot}} \right)\Phi\left(-\frac{\tau}{\sigma_{i}} \right) + \left[-O\left(\frac{n \log^2d}{ \ell^{3}}\right), 0 \right]
\end{split}
\end{equation}

Using results developed in Lemma~\ref{lem:boundSigma}, we have (see {\color{red} Corollary~\ref{cor:alphasigma} in Appendix XX}). 
\begin{equation}\label{eqn:var}
   \frac{\alpha^2 \sigma^2_i}{\sigma^2_j} =_p O\left(\frac{\log^2 d}{\ell} \right)
\quad \mbox{ and } \quad 
    \frac{\alpha^2}{\sigma^2_{i,\bot}} =_p O\left(\frac{n \log^2 d}{\ell^{3}}\right)
\end{equation} 
(\ref{eqn:var}) and Lemma (\ref{lem:calculate}) implies (\ref{eqn:decouplebnd}). 


\myparab{Putting everything together.} One can see that $I(|\hat \Sigma_{i,k} > \tau| \wedge |\hat \Sigma_{j,k}| > \tau)$ are independent for different $k$ conditioned on $\calf$. 
We may then apply Proposition~\ref{prop:interaction} and a Chernoff bound to show that $|N(i) \cap N(j)|$ is large when $i$ and $j$ are in the same block and is small otherwise. See {\color{red} Appendix X} for a full analysis. This completes the proof for the part 1 of Theorem~\ref{thm:main}. 

\myparab{Simple recursive trick.} 
Now we outline the recursive trick that enables us to tackle the case where $\ell_{\max} / \ell_{\min} = \omega(1)$. {\color{red} Appendix X gives the detail.}

\begin{corollary}\label{cor:recursive} Consider the block discovery problem, where $\ell_1 \geq \dots \geq \ell_k = \Omega(d^{\frac 2 3}\log^2d)$. There exists an efficient algorithm that identify all blocks with overwhelming probability. 
\end{corollary}

First, note that $\proc{Find-Blocks}$ still produce useful information when the $\ell_1 / \ell_k > 2$. With minimum modification, the algorithm (see $\proc{Robust-Find-Blocks}$ in Fig.~\ref{fig:robustneighbor_algo}, Sec.~\ref{asec:recursive}) can identify all blocks of size $\geq \ell/2$ as well as the union of all remaining blocks. 

Our recursive algorithm works as follows (See also $\proc{Recursive-Find-Blocks}$ in Fig.~\ref{fig:recursiveneighbor_algo}, Sec.~\ref{asec:recursive}): we first use $\proc{Robust-Find-Blocks}$ to find all blocks of size $\geq \ell/2$. Then we remove the columns of $\mY$ that correspond to blocks that are already found. In this case, $n$ is still a constant times larger than the largest block. So we may recurse and solve the smaller problems, until all blocks are detected. 
 